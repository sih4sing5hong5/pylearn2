!obj:pylearn2.train.Train {
    # For this example, we will train on a binarized version of MNIST.
    # We binarize by drawing samples--if an MNIST pixel is set to 0.9,
    # we make binary pixel that is 1 with probability 0.9. We redo the
    # sampling every time the example is presented to the learning
    # algorithm.
    # In pylearn2, we do this by making a Binarizer dataset. The Binarizer
    # is a dataset that can draw samples like this in terms of any
    # input dataset with values in [0,1].
    dataset: &data !obj:pylearn2.datasets.binarizer.Binarizer {
        # We use the "raw" tag to specify the underlying dataset defining
        # the sampling probabilities should be MNIST.
        raw: &raw_train !obj:pylearn2.datasets.mnist.MNIST {
            which_set: "train",
        }
    },
 
    # Here we specify the model to train as being an MLP
    model: !obj:pylearn2.models.mlp.MLP {
        layers : [
            # To make this baseline simple to run, we use a very small and cheap convolutional
            # network. It only has one hidden layer, consisting of rectifier units with spatial
            # max pooling.
            !obj:pylearn2.models.mlp.PretrainedLayer {
                layer_name: 'h0',
                layer_content: !pkl: "rbm1.pkl"
            },
            !obj:pylearn2.models.mlp.PretrainedLayer {
                layer_name: 'h1',
                layer_content: !pkl: "rbm2.pkl"
            },
            !obj:pylearn2.models.mlp.Softmax {
                layer_name: 'y',
                # The classes are unbalanced. Set the bias parameters of the softmax regression
                # to make the model start with the right marginals. This should speed convergence
                # of the training algorithm.
                #init_bias_target_marginals: *train,
                irange: .0,
                # There are ten different digitss to learn to recognize, i.e., 10 class labels
                n_classes: 10
            }
        ],
        nvis: 784,
    },
    # We train using SGD and momentum
    algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
        batch_size: 100,
        learning_rate: .001,
        init_momentum: .5,
        # We monitor how well we're doing during training on a validation set
        monitoring_dataset:
           {
                'valid' : !obj:pylearn2.datasets.binarizer.Binarizer {
                   raw: *raw_train,
                }
            },
        # We stop when validation set classification error hasn't decreased for 10 epochs
        termination_criterion: !obj:pylearn2.termination_criteria.MonitorBased {
            channel_name: "valid_y_misclass",
            prop_decrease: 0.1,
            N: 1
        },
    },
    # We save the model whenever we improve on the validation set classification error
    extensions: [
        !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest {
             channel_name: 'valid_y_misclass',
             save_path: "${PYLEARN2_TRAIN_FILE_FULL_STEM}_best.pkl"
        },
    ],
}
